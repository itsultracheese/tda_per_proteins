{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from scripts.helper_scripts.f1_max_score import count_f1_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = ''\n",
    "test_folder = ''\n",
    "valid_folder = ''\n",
    "\n",
    "files_train = os.listdir(train_folder)\n",
    "files_test = os.listdir(test_folder)\n",
    "files_valid = os.listdir(valid_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "849da3d57b7c48f2a88d0408e3971499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features_train = []\n",
    "labels_mf_train = []\n",
    "labels_cc_train = []\n",
    "labels_bp_train = []\n",
    "\n",
    "for file in tqdm(files_train):\n",
    "    with open(os.path.join(train_folder, file), 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        for key in data.keys():\n",
    "            features_train.append(data[key]['embeddings'].mean(axis=0))\n",
    "            labels_mf_train.append(data[key]['label_MF'])\n",
    "            labels_cc_train.append(data[key]['label_CC'])\n",
    "            labels_bp_train.append(data[key]['label_BP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af394eb9f3b043faac03fa4bc63ee8ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features_test = []\n",
    "labels_mf_test = []\n",
    "labels_cc_test = []\n",
    "labels_bp_test = []\n",
    "for file in tqdm(files_test):\n",
    "    with open(os.path.join(test_folder, file), 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        for key in data.keys():\n",
    "            features_test.append(data[key]['embeddings'].mean(axis=0))\n",
    "            labels_mf_test.append(data[key]['label_MF'])\n",
    "            labels_cc_test.append(data[key]['label_CC'])\n",
    "            labels_bp_test.append(data[key]['label_BP'])\n",
    "\n",
    "y_test_mf = np.array(labels_mf_test)\n",
    "y_test_cc = np.array(labels_cc_test)\n",
    "y_test_bp = np.array(labels_bp_test)\n",
    "\n",
    "features_valid = []\n",
    "labels_mf_valid = []\n",
    "labels_cc_valid = []\n",
    "labels_bp_valid = []\n",
    "for file in tqdm(files_valid):\n",
    "    with open(os.path.join(valid_folder, file), 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        for key in data.keys():\n",
    "            features_valid.append(data[key]['averaged_embedding'])\n",
    "            labels_mf_valid.append(data[key]['label_MF'])\n",
    "            labels_cc_valid.append(data[key]['label_CC'])\n",
    "            labels_bp_valid.append(data[key]['label_BP'])\n",
    "\n",
    "y_valid_mf = np.array(labels_mf_valid)\n",
    "y_valid_cc = np.array(labels_cc_valid)\n",
    "y_valid_bp = np.array(labels_bp_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3350"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score MF, max: 0.599583089351654\n"
     ]
    }
   ],
   "source": [
    "clf_mf = MultiOutputClassifier(KNeighborsClassifier(n_neighbors=3, weights='distance')).fit(features_train, labels_mf_train)\n",
    "preds_mf = clf_mf.predict_proba(features_test)\n",
    "print(f'F1 score MF, max: {count_f1_max(np.array([x[:, 1] if x.shape[1] == 2 else np.zeros(3350) for x in preds_mf]).T, y_test_mf)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score CC, max: 0.481135755777359\n"
     ]
    }
   ],
   "source": [
    "clf_cc = MultiOutputClassifier(KNeighborsClassifier(n_neighbors=3, weights='distance')).fit(features_train, labels_cc_train)\n",
    "preds_cc = clf_cc.predict_proba(features_test)\n",
    "print(f'F1 score CC, max: {count_f1_max(np.array([x[:, 1] if x.shape[1] == 2 else np.zeros(3350) for x in preds_cc]).T, y_test_cc)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score BP, max: 0.47102034091949463\n"
     ]
    }
   ],
   "source": [
    "clf_bp = MultiOutputClassifier(KNeighborsClassifier(n_neighbors=3)).fit(features_train, labels_bp_train)\n",
    "preds_bp = clf_bp.predict_proba(features_test)\n",
    "print(f'F1 score BP, max: {count_f1_max(np.array([x[:, 1] if x.shape[1] == 2 else np.zeros(3350) for x in preds_bp]).T, y_test_bp)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross-val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score MF, max: 0.2773600876331329, std: 0.01475842601081803\n",
      "F1 score CC, max: 0.18349820375442505, std: 0.018393149493804224\n",
      "F1 score BP, max: 0.25529301166534424, std: 0.014278667036727534\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=92)\n",
    "\n",
    "f1_scores_mf = []\n",
    "f1_scores_cc = []\n",
    "f1_scores_bp = []\n",
    "\n",
    "for train_index, test_index in kf.split(features_valid):\n",
    "    X_train = [features_valid[i] for i in train_index]\n",
    "    X_test = [features_valid[i] for i in test_index]\n",
    "    y_mf = y_valid_mf[train_index]\n",
    "    y_mf_t = y_valid_mf[test_index]\n",
    "    y_cc = y_valid_cc[train_index]\n",
    "    y_cc_t = y_valid_cc[test_index]\n",
    "    y_bp = y_valid_bp[train_index]\n",
    "    y_bp_t = y_valid_bp[test_index]\n",
    "\n",
    "    n_samples = len(y_mf_t)\n",
    "\n",
    "    clf_mf = MultiOutputClassifier(KNeighborsClassifier(n_neighbors=3)).fit(X_train, y_mf)\n",
    "    preds_mf = clf_mf.predict_proba(X_test)\n",
    "    f1_scores_mf.append(count_f1_max(np.array([x[:, 1] if x.shape[1] == 2 else np.zeros(n_samples) for x in preds_mf]).T, y_mf_t))\n",
    "\n",
    "    clf_cc = MultiOutputClassifier(KNeighborsClassifier(n_neighbors=3)).fit(X_train, y_cc)\n",
    "    preds_cc = clf_cc.predict_proba(X_test)\n",
    "    f1_scores_cc.append(count_f1_max(np.array([x[:, 1] if x.shape[1] == 2 else np.zeros(n_samples) for x in preds_cc]).T, y_cc_t))\n",
    "\n",
    "    clf_bp = MultiOutputClassifier(KNeighborsClassifier(n_neighbors=3)).fit(X_train, y_bp)\n",
    "    preds_bp = clf_bp.predict_proba(X_test)\n",
    "    f1_scores_bp.append(count_f1_max(np.array([x[:, 1] if x.shape[1] == 2 else np.zeros(n_samples) for x in preds_bp]).T, y_bp_t))\n",
    "\n",
    "print(f'F1 score MF, max: {np.mean(f1_scores_mf)}, std: {np.std(f1_scores_mf)}')\n",
    "print(f'F1 score CC, max: {np.mean(f1_scores_cc)}, std: {np.std(f1_scores_cc)}')\n",
    "print(f'F1 score BP, max: {np.mean(f1_scores_bp)}, std: {np.std(f1_scores_bp)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Baseline, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(512, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, cuda=True):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.cpu().item()\n",
    "    return train_loss / len(train_loader)\n",
    "\n",
    "def evaluate(model, test_loader, criterion, cuda=True, length=1261):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    preds = []\n",
    "    labels_gt = []\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(test_loader):\n",
    "            if cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            outputs = model(inputs)\n",
    "            preds.extend(F.sigmoid(outputs).cpu().numpy())\n",
    "            labels_gt.extend(labels.cpu().numpy())\n",
    "    return count_f1_max(np.array(preds).reshape(-1, length), np.array(labels_gt).reshape(-1, length))\n",
    "\n",
    "def train(model, train_loader, test_loader, optimizer, criterion, epochs, cuda=True, scheduler=None):\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, cuda)\n",
    "        valid_f1 = evaluate(model, test_loader, criterion, cuda)\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(valid_f1)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Valid F1: {valid_f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78442f28582e45bcbf58f266040686de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1261 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "files_valid = os.listdir('/media/ssd-3t-2/amiftakhova/tda_proteins/avg_embeddings_valid')\n",
    "features_valid = []\n",
    "labels_mf_valid = []\n",
    "labels_cc_valid = []\n",
    "labels_bp_valid = []\n",
    "for file in tqdm(files_valid):\n",
    "    with open('/media/ssd-3t-2/amiftakhova/tda_proteins/avg_embeddings_valid/'+file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        for key in data.keys():\n",
    "            features_valid.append(data[key]['averaged_embedding'])\n",
    "            labels_mf_valid.append(data[key]['label_MF'])\n",
    "            labels_cc_valid.append(data[key]['label_CC'])\n",
    "            labels_bp_valid.append(data[key]['label_BP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a52fe57712a4bb4b5c75657d586735f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.1566, Valid F1: 0.3343\n",
      "Epoch 2/50, Train Loss: 0.0297, Valid F1: 0.5074\n",
      "Epoch 3/50, Train Loss: 0.0236, Valid F1: 0.5798\n",
      "Epoch 4/50, Train Loss: 0.0198, Valid F1: 0.6234\n",
      "Epoch 5/50, Train Loss: 0.0175, Valid F1: 0.6698\n",
      "Epoch 6/50, Train Loss: 0.0160, Valid F1: 0.6832\n",
      "Epoch 7/50, Train Loss: 0.0147, Valid F1: 0.7056\n",
      "Epoch 8/50, Train Loss: 0.0139, Valid F1: 0.7166\n",
      "Epoch 9/50, Train Loss: 0.0131, Valid F1: 0.7277\n",
      "Epoch 10/50, Train Loss: 0.0124, Valid F1: 0.7389\n",
      "Epoch 11/50, Train Loss: 0.0118, Valid F1: 0.7363\n",
      "Epoch 12/50, Train Loss: 0.0112, Valid F1: 0.7516\n",
      "Epoch 13/50, Train Loss: 0.0107, Valid F1: 0.7557\n",
      "Epoch 14/50, Train Loss: 0.0103, Valid F1: 0.7695\n",
      "Epoch 15/50, Train Loss: 0.0098, Valid F1: 0.7696\n",
      "Epoch 16/50, Train Loss: 0.0095, Valid F1: 0.7749\n",
      "Epoch 17/50, Train Loss: 0.0093, Valid F1: 0.7743\n",
      "Epoch 18/50, Train Loss: 0.0090, Valid F1: 0.7734\n",
      "Epoch 19/50, Train Loss: 0.0087, Valid F1: 0.7832\n",
      "Epoch 20/50, Train Loss: 0.0085, Valid F1: 0.7813\n",
      "Epoch 21/50, Train Loss: 0.0082, Valid F1: 0.7820\n",
      "Epoch 22/50, Train Loss: 0.0081, Valid F1: 0.7808\n",
      "Epoch 23/50, Train Loss: 0.0078, Valid F1: 0.7949\n",
      "Epoch 24/50, Train Loss: 0.0077, Valid F1: 0.7960\n",
      "Epoch 25/50, Train Loss: 0.0075, Valid F1: 0.7917\n",
      "Epoch 26/50, Train Loss: 0.0075, Valid F1: 0.8018\n",
      "Epoch 27/50, Train Loss: 0.0072, Valid F1: 0.7993\n",
      "Epoch 28/50, Train Loss: 0.0070, Valid F1: 0.7976\n",
      "Epoch 29/50, Train Loss: 0.0068, Valid F1: 0.7961\n",
      "Epoch 30/50, Train Loss: 0.0067, Valid F1: 0.8016\n",
      "Epoch 31/50, Train Loss: 0.0067, Valid F1: 0.8009\n",
      "Epoch 32/50, Train Loss: 0.0066, Valid F1: 0.8066\n",
      "Epoch 33/50, Train Loss: 0.0064, Valid F1: 0.8063\n",
      "Epoch 34/50, Train Loss: 0.0063, Valid F1: 0.8034\n",
      "Epoch 35/50, Train Loss: 0.0063, Valid F1: 0.8051\n",
      "Epoch 36/50, Train Loss: 0.0061, Valid F1: 0.8086\n",
      "Epoch 37/50, Train Loss: 0.0060, Valid F1: 0.8077\n",
      "Epoch 38/50, Train Loss: 0.0060, Valid F1: 0.8135\n",
      "Epoch 39/50, Train Loss: 0.0060, Valid F1: 0.8088\n",
      "Epoch 40/50, Train Loss: 0.0058, Valid F1: 0.8137\n",
      "Epoch 41/50, Train Loss: 0.0056, Valid F1: 0.8136\n",
      "Epoch 42/50, Train Loss: 0.0056, Valid F1: 0.8121\n",
      "Epoch 43/50, Train Loss: 0.0056, Valid F1: 0.8174\n",
      "Epoch 44/50, Train Loss: 0.0055, Valid F1: 0.8094\n",
      "Epoch 45/50, Train Loss: 0.0055, Valid F1: 0.8131\n",
      "Epoch 46/50, Train Loss: 0.0054, Valid F1: 0.8100\n",
      "Epoch 47/50, Train Loss: 0.0052, Valid F1: 0.8177\n",
      "Epoch 48/50, Train Loss: 0.0053, Valid F1: 0.8178\n",
      "Epoch 49/50, Train Loss: 0.0053, Valid F1: 0.8200\n",
      "Epoch 50/50, Train Loss: 0.0053, Valid F1: 0.8196\n"
     ]
    }
   ],
   "source": [
    "LR = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.Tensor(features_train), torch.Tensor(labels_mf_train)), batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.Tensor(features_valid), torch.Tensor(labels_mf_valid)), batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.Tensor(features_test), torch.Tensor(labels_mf_test)), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "out_features = len(y_test_mf[0])\n",
    "model_mf = Baseline(1280, out_features).cuda()\n",
    "optimizer = torch.optim.Adam(model_mf.parameters(), lr=LR)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "\n",
    "train(model_mf, train_loader, valid_loader, optimizer, criterion, epochs=50)\n",
    "\n",
    "f1_test = evaluate(model_mf, test_loader, criterion, cuda=True, length=len(y_test_mf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6438495516777039"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa7b32aa411460bad52360f8d9801ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.1477, Valid F1: 0.3634\n",
      "Epoch 2/50, Train Loss: 0.0235, Valid F1: 0.4046\n",
      "Epoch 3/50, Train Loss: 0.0212, Valid F1: 0.4563\n",
      "Epoch 4/50, Train Loss: 0.0193, Valid F1: 0.4678\n",
      "Epoch 5/50, Train Loss: 0.0185, Valid F1: 0.4849\n",
      "Epoch 6/50, Train Loss: 0.0170, Valid F1: 0.5005\n",
      "Epoch 7/50, Train Loss: 0.0163, Valid F1: 0.5233\n",
      "Epoch 8/50, Train Loss: 0.0155, Valid F1: 0.5283\n",
      "Epoch 9/50, Train Loss: 0.0149, Valid F1: 0.5203\n",
      "Epoch 10/50, Train Loss: 0.0144, Valid F1: 0.5495\n",
      "Epoch 11/50, Train Loss: 0.0139, Valid F1: 0.5368\n",
      "Epoch 12/50, Train Loss: 0.0133, Valid F1: 0.5602\n",
      "Epoch 13/50, Train Loss: 0.0128, Valid F1: 0.5699\n",
      "Epoch 14/50, Train Loss: 0.0124, Valid F1: 0.5789\n",
      "Epoch 15/50, Train Loss: 0.0121, Valid F1: 0.5930\n",
      "Epoch 16/50, Train Loss: 0.0116, Valid F1: 0.5766\n",
      "Epoch 17/50, Train Loss: 0.0113, Valid F1: 0.5774\n",
      "Epoch 18/50, Train Loss: 0.0109, Valid F1: 0.5865\n",
      "Epoch 19/50, Train Loss: 0.0106, Valid F1: 0.5934\n",
      "Epoch 20/50, Train Loss: 0.0103, Valid F1: 0.6054\n",
      "Epoch 21/50, Train Loss: 0.0101, Valid F1: 0.5921\n",
      "Epoch 22/50, Train Loss: 0.0098, Valid F1: 0.6144\n",
      "Epoch 23/50, Train Loss: 0.0095, Valid F1: 0.6099\n",
      "Epoch 24/50, Train Loss: 0.0093, Valid F1: 0.6090\n",
      "Epoch 25/50, Train Loss: 0.0090, Valid F1: 0.5966\n",
      "Epoch 26/50, Train Loss: 0.0088, Valid F1: 0.6096\n",
      "Epoch 27/50, Train Loss: 0.0087, Valid F1: 0.6259\n",
      "Epoch 28/50, Train Loss: 0.0084, Valid F1: 0.6049\n",
      "Epoch 29/50, Train Loss: 0.0086, Valid F1: 0.6228\n",
      "Epoch 30/50, Train Loss: 0.0081, Valid F1: 0.6180\n",
      "Epoch 31/50, Train Loss: 0.0078, Valid F1: 0.6225\n",
      "Epoch 32/50, Train Loss: 0.0078, Valid F1: 0.6223\n",
      "Epoch 33/50, Train Loss: 0.0077, Valid F1: 0.6296\n",
      "Epoch 34/50, Train Loss: 0.0075, Valid F1: 0.6131\n",
      "Epoch 35/50, Train Loss: 0.0073, Valid F1: 0.6187\n",
      "Epoch 36/50, Train Loss: 0.0073, Valid F1: 0.6346\n",
      "Epoch 37/50, Train Loss: 0.0071, Valid F1: 0.6280\n",
      "Epoch 38/50, Train Loss: 0.0069, Valid F1: 0.6236\n",
      "Epoch 39/50, Train Loss: 0.0069, Valid F1: 0.6370\n",
      "Epoch 40/50, Train Loss: 0.0068, Valid F1: 0.6293\n",
      "Epoch 41/50, Train Loss: 0.0066, Valid F1: 0.6317\n",
      "Epoch 42/50, Train Loss: 0.0066, Valid F1: 0.6320\n",
      "Epoch 43/50, Train Loss: 0.0065, Valid F1: 0.6292\n",
      "Epoch 44/50, Train Loss: 0.0065, Valid F1: 0.6280\n",
      "Epoch 45/50, Train Loss: 0.0063, Valid F1: 0.6431\n",
      "Epoch 46/50, Train Loss: 0.0062, Valid F1: 0.6437\n",
      "Epoch 47/50, Train Loss: 0.0062, Valid F1: 0.6482\n",
      "Epoch 48/50, Train Loss: 0.0061, Valid F1: 0.6515\n",
      "Epoch 49/50, Train Loss: 0.0059, Valid F1: 0.6450\n",
      "Epoch 50/50, Train Loss: 0.0058, Valid F1: 0.6442\n"
     ]
    }
   ],
   "source": [
    "LR = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.Tensor(features_train), torch.Tensor(labels_cc_train)), batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.Tensor(features_valid), torch.Tensor(labels_cc_valid)), batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.Tensor(features_test), torch.Tensor(labels_cc_test)), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "out_features = len(y_test_cc[0])\n",
    "model_cc = Baseline(1280, out_features).cuda()\n",
    "optimizer = torch.optim.Adam(model_cc.parameters(), lr=LR)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "\n",
    "train(model_cc, train_loader, valid_loader, optimizer, criterion, epochs=50)\n",
    "\n",
    "f1_test = evaluate(model_cc, test_loader, criterion, cuda=True, length=len(y_test_cc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48056766390800476"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5cb41fb91f44b8c87e061e0db4200d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.1517, Valid F1: 0.2473\n",
      "Epoch 2/50, Train Loss: 0.0277, Valid F1: 0.3073\n",
      "Epoch 3/50, Train Loss: 0.0252, Valid F1: 0.3459\n",
      "Epoch 4/50, Train Loss: 0.0237, Valid F1: 0.3838\n",
      "Epoch 5/50, Train Loss: 0.0221, Valid F1: 0.4101\n",
      "Epoch 6/50, Train Loss: 0.0209, Valid F1: 0.4329\n",
      "Epoch 7/50, Train Loss: 0.0201, Valid F1: 0.4520\n",
      "Epoch 8/50, Train Loss: 0.0191, Valid F1: 0.4715\n",
      "Epoch 9/50, Train Loss: 0.0182, Valid F1: 0.4786\n",
      "Epoch 10/50, Train Loss: 0.0175, Valid F1: 0.4977\n",
      "Epoch 11/50, Train Loss: 0.0170, Valid F1: 0.4948\n",
      "Epoch 12/50, Train Loss: 0.0163, Valid F1: 0.5209\n",
      "Epoch 13/50, Train Loss: 0.0159, Valid F1: 0.5306\n",
      "Epoch 14/50, Train Loss: 0.0153, Valid F1: 0.5383\n",
      "Epoch 15/50, Train Loss: 0.0149, Valid F1: 0.5395\n",
      "Epoch 16/50, Train Loss: 0.0144, Valid F1: 0.5481\n",
      "Epoch 17/50, Train Loss: 0.0141, Valid F1: 0.5585\n",
      "Epoch 18/50, Train Loss: 0.0137, Valid F1: 0.5617\n",
      "Epoch 19/50, Train Loss: 0.0134, Valid F1: 0.5659\n",
      "Epoch 20/50, Train Loss: 0.0130, Valid F1: 0.5676\n",
      "Epoch 21/50, Train Loss: 0.0128, Valid F1: 0.5739\n",
      "Epoch 22/50, Train Loss: 0.0127, Valid F1: 0.5807\n",
      "Epoch 23/50, Train Loss: 0.0121, Valid F1: 0.5813\n",
      "Epoch 24/50, Train Loss: 0.0120, Valid F1: 0.5826\n",
      "Epoch 25/50, Train Loss: 0.0117, Valid F1: 0.5952\n",
      "Epoch 26/50, Train Loss: 0.0114, Valid F1: 0.5866\n",
      "Epoch 27/50, Train Loss: 0.0113, Valid F1: 0.5874\n",
      "Epoch 28/50, Train Loss: 0.0110, Valid F1: 0.5983\n",
      "Epoch 29/50, Train Loss: 0.0109, Valid F1: 0.5990\n",
      "Epoch 30/50, Train Loss: 0.0107, Valid F1: 0.6045\n",
      "Epoch 31/50, Train Loss: 0.0104, Valid F1: 0.6055\n",
      "Epoch 32/50, Train Loss: 0.0104, Valid F1: 0.6030\n",
      "Epoch 33/50, Train Loss: 0.0103, Valid F1: 0.6048\n",
      "Epoch 34/50, Train Loss: 0.0101, Valid F1: 0.6081\n",
      "Epoch 35/50, Train Loss: 0.0099, Valid F1: 0.6114\n",
      "Epoch 36/50, Train Loss: 0.0098, Valid F1: 0.6114\n",
      "Epoch 37/50, Train Loss: 0.0096, Valid F1: 0.6161\n",
      "Epoch 38/50, Train Loss: 0.0094, Valid F1: 0.6125\n",
      "Epoch 39/50, Train Loss: 0.0092, Valid F1: 0.6209\n",
      "Epoch 40/50, Train Loss: 0.0094, Valid F1: 0.6146\n",
      "Epoch 41/50, Train Loss: 0.0091, Valid F1: 0.6214\n",
      "Epoch 42/50, Train Loss: 0.0090, Valid F1: 0.6215\n",
      "Epoch 43/50, Train Loss: 0.0090, Valid F1: 0.6180\n",
      "Epoch 44/50, Train Loss: 0.0088, Valid F1: 0.6216\n",
      "Epoch 45/50, Train Loss: 0.0087, Valid F1: 0.6202\n",
      "Epoch 46/50, Train Loss: 0.0087, Valid F1: 0.6208\n",
      "Epoch 47/50, Train Loss: 0.0086, Valid F1: 0.6202\n",
      "Epoch 48/50, Train Loss: 0.0085, Valid F1: 0.6173\n",
      "Epoch 49/50, Train Loss: 0.0085, Valid F1: 0.6330\n",
      "Epoch 50/50, Train Loss: 0.0083, Valid F1: 0.6268\n"
     ]
    }
   ],
   "source": [
    "LR = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.Tensor(features_train), torch.Tensor(labels_bp_train)), batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.Tensor(features_valid), torch.Tensor(labels_bp_valid)), batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.Tensor(features_test), torch.Tensor(labels_bp_test)), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "out_features = len(y_test_bp[0])\n",
    "model_bp = Baseline(1280, out_features).cuda()\n",
    "optimizer = torch.optim.Adam(model_bp.parameters(), lr=LR)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "\n",
    "train(model_bp, train_loader, valid_loader, optimizer, criterion, epochs=50)\n",
    "\n",
    "f1_test = evaluate(model_bp, test_loader, criterion, cuda=True, length=len(y_test_bp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45557740330696106"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
